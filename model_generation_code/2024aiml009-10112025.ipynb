{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de2ae2-9719-4502-8fdb-5c4cfc33dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Training and evaluating individual models...\n",
      "\n",
      "â–¶ Training CatBoost...\n"
     ]
    }
   ],
   "source": [
    "# train_high_accuracy_top3_voting.py\n",
    "import os, re, warnings, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "DATA_PATH = \"data/AP_data.csv\"\n",
    "DIST_PATH = \"data/AP_district_level_master.csv\"\n",
    "USE_TOP_N_CLASSES = True\n",
    "TOP_N = 12\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "CB_PARAMS = dict(\n",
    "    iterations=1200,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    "    l2_leaf_reg=3,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    subsample=0.8,\n",
    "    random_seed=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "MODEL_OUT_CAT = \"serving_catboost_topN.pkl\"\n",
    "MODEL_OUT_VOTE = \"soft_voting_model.pkl\"\n",
    "REPORT_OUT = \"training_report_topN_voting.txt\"\n",
    "COMPARE_CSV = \"model_comparison_top3_voting.csv\"\n",
    "\n",
    "# ===================== LOAD & CLEAN =====================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "dist_master = pd.read_csv(DIST_PATH)\n",
    "\n",
    "df = df.rename(columns={\"Extent\\n(AC)\": \"Farm_Acres\", \"Crop before\": \"Crop_Sown\"})\n",
    "df[\"District\"] = df[\"District\"].replace({\n",
    "    \"Anantapur\": \"Ananthapur\",\n",
    "    \"S.P.S.Nellore\": \"Nellore\",\n",
    "    \"S.P.S. Nellore\": \"Nellore\",\n",
    "    \"Kadapa YSR\": \"Kadapa\"\n",
    "})\n",
    "\n",
    "if {\"District\",\"Kharif_rain\",\"Rabi_rain\",\"Zaid_rain\"}.issubset(dist_master.columns):\n",
    "    rain_df = dist_master[[\"District\",\"Kharif_rain\",\"Rabi_rain\",\"Zaid_rain\"]].drop_duplicates()\n",
    "    df = df.merge(rain_df, on=\"District\", how=\"left\")\n",
    "\n",
    "drop_cols = [\n",
    "    \"Sl no\",\"Date\",\"Farmer No\",\"Macro/ Micro nutrient\",\"Farmer Name\",\n",
    "    \"Fathers Name\",\"Time\",\"Recommended Sowing Time\",\"Survey No.\",\n",
    "    \"Latitude\",\"Longitude\",\"Farm_Acres\"\n",
    "]\n",
    "df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "for col in [\"OC\",\"Avail-S\",\"Avail-B\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "MASTER_SOIL = [\"Black\",\"Red\",\"Sandy\",\"Loam\",\"Clay\",\"Brown\",\"Yellow\",\"White\",\"Laterite\",\"Saline\",\"Alkaline\",\"Alluvial\",\"Gravel/Stony\",\"Mixed\",\"Other\"]\n",
    "\n",
    "def _clean_text(s): return re.sub(r\"[^a-z\\s\\+\\-]\", \"\", str(s).lower().replace(\"soil\", \"\").strip())\n",
    "def standardize_soil(raw):\n",
    "    if not isinstance(raw, str) or not raw.strip(): return \"Unknown\"\n",
    "    txt = _clean_text(raw)\n",
    "    match, score, _ = process.extractOne(txt, MASTER_SOIL, scorer=fuzz.WRatio)\n",
    "    return match if score >= 80 else \"Unknown\"\n",
    "def standardize_crop(raw):\n",
    "    if not isinstance(raw, str) or not raw.strip(): return \"Other\"\n",
    "    txt = raw.lower()\n",
    "    aliases = {\n",
    "        \"paddy\":\"Rice\",\"vari\":\"Rice\",\"rice\":\"Rice\",\n",
    "        \"maize\":\"Maize\",\"sweetcorn\":\"Maize\",\n",
    "        \"ground nut\":\"Groundnut\",\"groundnut\":\"Groundnut\",\"g.nut\":\"Groundnut\",\n",
    "        \"cotton\":\"Cotton\",\"castor\":\"Castor\",\"sesamum\":\"Sesame\",\"sesame\":\"Sesame\",\n",
    "        \"sunflower\":\"Sunflower\",\"soyabean\":\"Soyabean\",\"soybean\":\"Soyabean\",\n",
    "        \"chilli\":\"Chilli\",\"chillies\":\"Chilli\",\"mirchi\":\"Chilli\",\n",
    "        \"tomato\":\"Tomato\",\"brinjal\":\"Brinjal\",\"okra\":\"Okra\",\"benda\":\"Okra\",\n",
    "        \"ragi\":\"Ragi\",\"sorghum\":\"Sorghum\",\"jowar\":\"Sorghum\",\"jonna\":\"Sorghum\",\n",
    "        \"bajra\":\"Pearl Millet\",\"korra\":\"Foxtail Millet\"\n",
    "    }\n",
    "    for k,v in aliases.items():\n",
    "        if k in txt: return v\n",
    "    return raw.title()\n",
    "\n",
    "df[\"Soil_Type_Standard\"] = df[\"Soil type\"].apply(standardize_soil)\n",
    "df[\"Crop_Sown_Standard\"] = df[\"Crop_Sown\"].apply(standardize_crop)\n",
    "\n",
    "num_cols = [c for c in [\n",
    "    \"pH\",\"EC\",\"OC\",\"Avail-P\",\"Exch-K\",\"Avail-Ca\",\"Avail-Mg\",\"Avail-S\",\n",
    "    \"Avail-Zn\",\"Avail-B\",\"Avail-Fe\",\"Avail-Cu\",\"Avail-Mn\",\n",
    "    \"Kharif_rain\",\"Rabi_rain\",\"Zaid_rain\"\n",
    "] if c in df.columns]\n",
    "cat_cols = [c for c in [\"District\",\"Soil_Type_Standard\"] if c in df.columns]\n",
    "target = \"Crop_Sown_Standard\"\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "def safe_mul(a,b): return pd.to_numeric(a, errors=\"coerce\").fillna(0)*pd.to_numeric(b, errors=\"coerce\").fillna(0)\n",
    "if \"pH\" in df and \"EC\" in df: df[\"pH_x_EC\"] = safe_mul(df[\"pH\"], df[\"EC\"])\n",
    "if \"OC\" in df and \"Avail-P\" in df: df[\"OC_x_AvailP\"] = safe_mul(df[\"OC\"], df[\"Avail-P\"])\n",
    "num_cols += [c for c in [\"pH_x_EC\",\"OC_x_AvailP\"] if c in df.columns]\n",
    "\n",
    "if USE_TOP_N_CLASSES:\n",
    "    top_classes = df[target].value_counts().head(TOP_N).index\n",
    "    df = df[df[target].isin(top_classes)].copy()\n",
    "\n",
    "for c in cat_cols: df[c] = df[c].astype(\"string\").fillna(\"Unknown\")\n",
    "for c in num_cols: df[c] = df[c].fillna(0).astype(float)\n",
    "\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "y = df[target].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n",
    "le = LabelEncoder().fit(y)\n",
    "y_true_int = le.transform(y_test)\n",
    "\n",
    "# ===================== CLONE-SAFE CATBOOST WRAPPER =====================\n",
    "class CatBoostAutoCat(CatBoostClassifier):\n",
    "    def __init__(self, cat_feature_names=None, **kwargs):\n",
    "        self.cat_feature_names = cat_feature_names\n",
    "        super().__init__(**kwargs)\n",
    "    def get_params(self, deep=True):\n",
    "        params = super().get_params(deep)\n",
    "        params[\"cat_feature_names\"] = self.cat_feature_names\n",
    "        return params\n",
    "    def set_params(self, **params):\n",
    "        if \"cat_feature_names\" in params:\n",
    "            self.cat_feature_names = params.pop(\"cat_feature_names\")\n",
    "        return super().set_params(**params)\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        if isinstance(X, pd.DataFrame) and self.cat_feature_names:\n",
    "            cf = [c for c in self.cat_feature_names if c in X.columns]\n",
    "            return super().fit(X, y, cat_features=cf, **fit_params)\n",
    "        return super().fit(X, y, **fit_params)\n",
    "\n",
    "# ===================== BASE MODELS =====================\n",
    "prep = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "])\n",
    "rf  = Pipeline([(\"prep\", prep), (\"clf\", RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE))])\n",
    "gb  = Pipeline([(\"prep\", prep), (\"clf\", GradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
    "knn = Pipeline([(\"prep\", prep), (\"clf\", KNeighborsClassifier(n_neighbors=15, weights=\"distance\"))])\n",
    "cb  = CatBoostAutoCat(cat_feature_names=cat_cols, **CB_PARAMS)\n",
    "\n",
    "# ===================== INDIVIDUAL MODEL EVALUATION =====================\n",
    "print(\"ðŸ”¹ Training and evaluating individual models...\")\n",
    "\n",
    "models = {\n",
    "    \"CatBoost\": cb,\n",
    "    \"RandomForest\": rf,\n",
    "    \"GradientBoosting\": gb,\n",
    "    \"KNN\": knn\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nâ–¶ Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    probs = model.predict_proba(X_test)\n",
    "    order = [np.where(model.classes_ == c)[0][0] for c in le.classes_]\n",
    "    probs_aligned = probs[:, order]\n",
    "\n",
    "    acc1  = top_k_accuracy_score(y_true_int, probs_aligned, k=1)\n",
    "    acc3  = top_k_accuracy_score(y_true_int, probs_aligned, k=3)\n",
    "    acc10 = top_k_accuracy_score(y_true_int, probs_aligned, k=min(10, probs_aligned.shape[1]))\n",
    "\n",
    "    print(f\"{name} â€” Top-1: {acc1:.4f} | Top-3: {acc3:.4f} | Top-10: {acc10:.4f}\")\n",
    "    results.append((name, acc1, acc3, acc10))\n",
    "\n",
    "# ===================== SOFT VOTING ENSEMBLE =====================\n",
    "print(\"\\nðŸ”¹ Training Soft VotingClassifier (CatBoost + RF + GB + KNN)â€¦\")\n",
    "\n",
    "voter = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"CatBoost\", cb),\n",
    "        (\"RandomForest\", rf),\n",
    "        (\"GradientBoosting\", gb),\n",
    "        (\"KNN\", knn)\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "voter.fit(X_train, y_train)\n",
    "\n",
    "# ===================== EVALUATION =====================\n",
    "probs_vote = voter.predict_proba(X_test)\n",
    "order = [np.where(voter.classes_ == c)[0][0] for c in le.classes_]\n",
    "probs_aligned = probs_vote[:, order]\n",
    "\n",
    "acc_top1  = top_k_accuracy_score(y_true_int, probs_aligned, k=1)\n",
    "acc_top3  = top_k_accuracy_score(y_true_int, probs_aligned, k=3)\n",
    "acc_top10 = top_k_accuracy_score(y_true_int, probs_aligned, k=min(10, probs_aligned.shape[1]))\n",
    "\n",
    "print(f\"\\nâœ… SoftVoting â€” Top-1: {acc_top1:.4f} | Top-3: {acc_top3:.4f} | Top-10: {acc_top10:.4f}\")\n",
    "\n",
    "# Save results to CSV for easy comparison\n",
    "compare_df = pd.DataFrame(results, columns=[\"Model\", \"Top-1\", \"Top-3\", \"Top-10\"])\n",
    "compare_df.loc[len(compare_df)] = [\"SoftVoting\", acc_top1, acc_top3, acc_top10]\n",
    "compare_df.to_csv(COMPARE_CSV, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Accuracy comparison saved to {COMPARE_CSV}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "joblib.dump({\n",
    "    \"model\": voter,\n",
    "    \"feature_names\": X.columns.tolist(),\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"num_cols\": num_cols,\n",
    "    \"label_encoder\": le\n",
    "}, MODEL_OUT_VOTE)\n",
    "print(\"ðŸ’¾ Saved model:\", MODEL_OUT_VOTE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06850aaa-4c74-456c-98ad-243dcc4d8e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
