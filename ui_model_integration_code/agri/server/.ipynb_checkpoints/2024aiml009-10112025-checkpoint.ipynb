{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b2243b-954e-44c7-af24-45872d427c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Training Soft VotingClassifier (CatBoost + RF + GB + KNN)â€¦\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <__main__.CatBoostAutoCat object at 0x000001CC9AC2AF90>, as the constructor either does not set or modifies parameter cat_feature_names",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 249\u001b[39m\n\u001b[32m    238\u001b[39m voter = VotingClassifier(\n\u001b[32m    239\u001b[39m     estimators=[\n\u001b[32m    240\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mCatBoost\u001b[39m\u001b[33m\"\u001b[39m, cb),\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m     voting=\u001b[33m\"\u001b[39m\u001b[33msoft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m )\n\u001b[32m    248\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”¹ Training Soft VotingClassifier (CatBoost + RF + GB + KNN)â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[43mvoter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:405\u001b[39m, in \u001b[36mVotingClassifier.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = \u001b[38;5;28mself\u001b[39m.le_.classes_\n\u001b[32m    403\u001b[39m transformed_y = \u001b[38;5;28mself\u001b[39m.le_.transform(y)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:99\u001b[39m, in \u001b[36m_BaseVoting.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fit_params:\n\u001b[32m     95\u001b[39m             routed_params[name].fit[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m] = fit_params[\n\u001b[32m     96\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m             ]\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVoting\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdrop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.named_estimators_ = Bunch()\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1911\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1908\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1910\u001b[39m \u001b[38;5;66;03m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_batches\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1913\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_tasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:80\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = \u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_with_config_and_warning_filters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarning_filters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:101\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fit_params:\n\u001b[32m     95\u001b[39m             routed_params[name].fit[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m] = fit_params[\n\u001b[32m     96\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m             ]\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ = Parallel(n_jobs=\u001b[38;5;28mself\u001b[39m.n_jobs)(\n\u001b[32m    100\u001b[39m     delayed(_fit_single_estimator)(\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    102\u001b[39m         X,\n\u001b[32m    103\u001b[39m         y,\n\u001b[32m    104\u001b[39m         fit_params=routed_params[name][\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    105\u001b[39m         message_clsname=\u001b[33m\"\u001b[39m\u001b[33mVoting\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         message=\u001b[38;5;28mself\u001b[39m._log_message(name, idx + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(clfs)),\n\u001b[32m    107\u001b[39m     )\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, clf) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(names, clfs))\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clf != \u001b[33m\"\u001b[39m\u001b[33mdrop\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m )\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.named_estimators_ = Bunch()\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:95\u001b[39m, in \u001b[36mclone\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_clone__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.isclass(estimator):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator.__sklearn_clone__()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:142\u001b[39m, in \u001b[36m_clone_parametrized\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m    140\u001b[39m     param2 = params_set[name]\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param2:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    143\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot clone object \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, as the constructor \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    144\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meither does not set or modifies parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (estimator, name)\n\u001b[32m    145\u001b[39m         )\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# _sklearn_output_config is used by `set_output` to configure the output\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# container of an estimator.\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_sklearn_output_config\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Cannot clone object <__main__.CatBoostAutoCat object at 0x000001CC9AC2AF90>, as the constructor either does not set or modifies parameter cat_feature_names"
     ]
    }
   ],
   "source": [
    "# train_high_accuracy_top3_voting.py\n",
    "import os, re, warnings, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import top_k_accuracy_score, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "DATA_PATH = \"data/AP_data.csv\"\n",
    "DIST_PATH = \"data/AP_district_level_master.csv\"\n",
    "USE_TOP_N_CLASSES = True\n",
    "TOP_N = 12\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "CB_PARAMS = dict(\n",
    "    iterations=1200,\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    "    l2_leaf_reg=3,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    subsample=0.8,\n",
    "    random_seed=RANDOM_STATE,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "MODEL_OUT_CAT = \"serving_catboost_topN.pkl\"\n",
    "MODEL_OUT_VOTE = \"soft_voting_model.pkl\"\n",
    "REPORT_OUT = \"training_report_topN_voting.txt\"\n",
    "COMPARE_CSV = \"model_comparison_top3_voting.csv\"\n",
    "\n",
    "# ===================== LOAD & CLEAN =====================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "dist_master = pd.read_csv(DIST_PATH)\n",
    "\n",
    "df = df.rename(columns={\"Extent\\n(AC)\": \"Farm_Acres\", \"Crop before\": \"Crop_Sown\"})\n",
    "df[\"District\"] = df[\"District\"].replace({\n",
    "    \"Anantapur\": \"Ananthapur\",\n",
    "    \"S.P.S.Nellore\": \"Nellore\",\n",
    "    \"S.P.S. Nellore\": \"Nellore\",\n",
    "    \"Kadapa YSR\": \"Kadapa\"\n",
    "})\n",
    "\n",
    "if {\"District\",\"Kharif_rain\",\"Rabi_rain\",\"Zaid_rain\"}.issubset(dist_master.columns):\n",
    "    rain_df = dist_master[[\"District\",\"Kharif_rain\",\"Rabi_rain\",\"Zaid_rain\"]].drop_duplicates()\n",
    "    df = df.merge(rain_df, on=\"District\", how=\"left\")\n",
    "\n",
    "drop_cols = [\n",
    "    \"Sl no\",\"Date\",\"Farmer No\",\"Macro/ Micro nutrient\",\"Farmer Name\",\n",
    "    \"Fathers Name\",\"Time\",\"Recommended Sowing Time\",\"Survey No.\",\n",
    "    \"Latitude\",\"Longitude\",\"Farm_Acres\"\n",
    "]\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "for col in [\"OC\",\"Avail-S\",\"Avail-B\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ===================== STANDARDIZATION =====================\n",
    "MASTER_SOIL = [\"Black\",\"Red\",\"Sandy\",\"Loam\",\"Clay\",\"Brown\",\"Yellow\",\"White\",\"Laterite\",\"Saline\",\"Alkaline\",\"Alluvial\",\"Gravel/Stony\",\"Mixed\",\"Other\"]\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"soil\", \"\", s)\n",
    "    s = re.sub(r\"[^a-z\\s\\+\\-]\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def standardize_soil(raw: str) -> str:\n",
    "    if not isinstance(raw, str) or not raw.strip():\n",
    "        return \"Unknown\"\n",
    "    txt = _clean_text(raw)\n",
    "    match, score, _ = process.extractOne(txt, MASTER_SOIL, scorer=fuzz.WRatio)\n",
    "    return match if score >= 80 else \"Unknown\"\n",
    "\n",
    "def standardize_crop(raw: str) -> str:\n",
    "    if not isinstance(raw, str) or not raw.strip():\n",
    "        return \"Other\"\n",
    "    txt = raw.lower()\n",
    "    aliases = {\n",
    "        \"paddy\":\"Rice\",\"vari\":\"Rice\",\"rice\":\"Rice\",\n",
    "        \"maize\":\"Maize\",\"sweetcorn\":\"Maize\",\n",
    "        \"ground nut\":\"Groundnut\",\"groundnut\":\"Groundnut\",\"g.nut\":\"Groundnut\",\n",
    "        \"cotton\":\"Cotton\",\"castor\":\"Castor\",\"sesamum\":\"Sesame\",\"sesame\":\"Sesame\",\n",
    "        \"sunflower\":\"Sunflower\",\"soyabean\":\"Soyabean\",\"soybean\":\"Soyabean\",\n",
    "        \"chilli\":\"Chilli\",\"chillies\":\"Chilli\",\"mirchi\":\"Chilli\",\n",
    "        \"tomato\":\"Tomato\",\"brinjal\":\"Brinjal\",\"okra\":\"Okra\",\"benda\":\"Okra\",\n",
    "        \"ragi\":\"Ragi\",\"sorghum\":\"Sorghum\",\"jowar\":\"Sorghum\",\"jonna\":\"Sorghum\",\n",
    "        \"bajra\":\"Pearl Millet\",\"korra\":\"Foxtail Millet\"\n",
    "    }\n",
    "    for k,v in aliases.items():\n",
    "        if k in txt: return v\n",
    "    return raw.title()\n",
    "\n",
    "df[\"Soil_Type_Standard\"] = df[\"Soil type\"].apply(standardize_soil)\n",
    "df[\"Crop_Sown_Standard\"] = df[\"Crop_Sown\"].apply(standardize_crop)\n",
    "\n",
    "# ===================== FEATURES =====================\n",
    "num_cols = [c for c in [\n",
    "    \"pH\",\"EC\",\"OC\",\"Avail-P\",\"Exch-K\",\"Avail-Ca\",\"Avail-Mg\",\"Avail-S\",\n",
    "    \"Avail-Zn\",\"Avail-B\",\"Avail-Fe\",\"Avail-Cu\",\"Avail-Mn\",\n",
    "    \"Kharif_rain\",\"Rabi_rain\",\"Zaid_rain\"\n",
    "] if c in df.columns]\n",
    "cat_cols = [c for c in [\"District\",\"Soil_Type_Standard\"] if c in df.columns]\n",
    "target = \"Crop_Sown_Standard\"\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(df[c].median())\n",
    "\n",
    "# Interactions\n",
    "def safe_mul(a,b): return pd.to_numeric(a, errors=\"coerce\").fillna(0)*pd.to_numeric(b, errors=\"coerce\").fillna(0)\n",
    "def add_interactions(d):\n",
    "    if \"pH\" in d and \"EC\" in d: d[\"pH_x_EC\"] = safe_mul(d[\"pH\"], d[\"EC\"])\n",
    "    if \"OC\" in d and \"Avail-P\" in d: d[\"OC_x_AvailP\"] = safe_mul(d[\"OC\"], d[\"Avail-P\"])\n",
    "    return d\n",
    "df = add_interactions(df)\n",
    "extra_nums = [c for c in [\"pH_x_EC\",\"OC_x_AvailP\"] if c in df.columns]\n",
    "num_cols = num_cols + extra_nums\n",
    "\n",
    "if USE_TOP_N_CLASSES:\n",
    "    top_classes = df[target].value_counts().head(TOP_N).index\n",
    "    df = df[df[target].isin(top_classes)].copy()\n",
    "\n",
    "# Clean types\n",
    "for c in cat_cols: df[c] = df[c].astype(\"string\").fillna(\"Unknown\")\n",
    "for c in num_cols: df[c] = df[c].fillna(0).astype(float)\n",
    "\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n",
    "le = LabelEncoder().fit(y)\n",
    "y_true_int = le.transform(y_test)\n",
    "\n",
    "# ===================== BASELINES / INDIVIDUAL MODELS =====================\n",
    "# CatBoost (solo) for comparison\n",
    "cat_idx = [X.columns.get_loc(c) for c in cat_cols]\n",
    "cb = CatBoostClassifier(**CB_PARAMS)\n",
    "cb.fit(Pool(X_train, y_train, cat_features=cat_idx))\n",
    "probs_cb = cb.predict_proba(X_test)\n",
    "classes_cb = cb.classes_\n",
    "order_cb = [np.where(classes_cb == c)[0][0] for c in le.classes_]\n",
    "probs_cb = probs_cb[:, order_cb]\n",
    "acc_cb = top_k_accuracy_score(y_true_int, probs_cb, k=3)\n",
    "\n",
    "# sklearn baselines\n",
    "prep = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "])\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=15, weights=\"distance\")\n",
    "}\n",
    "results, probs_dict = [(\"CatBoost\", acc_cb)], {\"CatBoost\": probs_cb}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([(\"prep\", prep), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    probs = pipe.predict_proba(X_test)\n",
    "    order = [np.where(pipe.named_steps[\"clf\"].classes_ == c)[0][0] for c in le.classes_]\n",
    "    probs = probs[:, order]\n",
    "    acc = top_k_accuracy_score(y_true_int, probs, k=3)\n",
    "    results.append((name, acc))\n",
    "    probs_dict[name] = probs\n",
    "\n",
    "# =====================  CATBOOST WRAPPER =====================\n",
    "# --- replace your previous CatBoostAutoCat with THIS version ---\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "\n",
    "class CatBoostAutoCat(CatBoostClassifier):\n",
    "    \"\"\"\n",
    "    CatBoost that survives sklearn cloning inside VotingClassifier and\n",
    "    always treats the given column NAMES as categorical when X is a DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_feature_names=None, **kwargs):\n",
    "        # store before super() so get_params sees it\n",
    "        self.cat_feature_names = list(cat_feature_names) if cat_feature_names else []\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    # make sure sklearn's clone() keeps our custom param\n",
    "    def get_params(self, deep=True):\n",
    "        params = super().get_params(deep=deep)\n",
    "        params[\"cat_feature_names\"] = self.cat_feature_names\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        if \"cat_feature_names\" in params:\n",
    "            self.cat_feature_names = list(params.pop(\"cat_feature_names\"))\n",
    "        return super().set_params(**params)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        cf = None\n",
    "        if isinstance(X, pd.DataFrame) and self.cat_feature_names:\n",
    "            # CatBoost accepts column NAMES for pandas.DataFrame\n",
    "            cf = [c for c in self.cat_feature_names if c in X.columns]\n",
    "        return super().fit(X, y, cat_features=cf, **fit_params)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# preprocessing for sklearn models\n",
    "prep = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    # if your sklearn version < 1.2, use `sparse=False` instead of `sparse_output=False`\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "rf  = Pipeline([(\"prep\", prep), (\"clf\", RandomForestClassifier(n_estimators=500, random_state=RANDOM_STATE))])\n",
    "gb  = Pipeline([(\"prep\", prep), (\"clf\", GradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
    "knn = Pipeline([(\"prep\", prep), (\"clf\", KNeighborsClassifier(n_neighbors=15, weights=\"distance\"))])\n",
    "\n",
    "# our CatBoost that auto-handles categorical cols\n",
    "cb  = CatBoostAutoCat(cat_feature_names=cat_cols, **CB_PARAMS)\n",
    "\n",
    "# SOFT VOTING ENSEMBLE (CatBoost + RF + GB + KNN)\n",
    "voter = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"CatBoost\", cb),\n",
    "        (\"RandomForest\", rf),\n",
    "        (\"GradientBoosting\", gb),\n",
    "        (\"KNN\", knn),\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ”¹ Training Soft VotingClassifier (CatBoost + RF + GB + KNN)â€¦\")\n",
    "voter.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c3d305-f481-42e7-8857-9bde746f1119",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VotingClassifier' object has no attribute 'estimators_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Top-1 / Top-3 / Top-10\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m y_pred_vote = \u001b[43mvoter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m probs_vote  = voter.predict_proba(X_test)\n\u001b[32m     11\u001b[39m le = LabelEncoder().fit(y)                 \u001b[38;5;66;03m# already in your script; reuse if present\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:422\u001b[39m, in \u001b[36mVotingClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    420\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.voting == \u001b[33m\"\u001b[39m\u001b[33msoft\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     maj = np.argmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 'hard' voting\u001b[39;00m\n\u001b[32m    425\u001b[39m     predictions = \u001b[38;5;28mself\u001b[39m._predict(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:463\u001b[39m, in \u001b[36mVotingClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[32m    450\u001b[39m \n\u001b[32m    451\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    461\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    462\u001b[39m avg = np.average(\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collect_probas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m0\u001b[39m, weights=\u001b[38;5;28mself\u001b[39m._weights_not_none\n\u001b[32m    464\u001b[39m )\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:438\u001b[39m, in \u001b[36mVotingClassifier._collect_probas\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    437\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray([clf.predict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m])\n",
      "\u001b[31mAttributeError\u001b[39m: 'VotingClassifier' object has no attribute 'estimators_'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate Top-1 / Top-3 / Top-10\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Top-1 / Top-3 / Top-10\n",
    "y_pred_vote = voter.predict(X_test)\n",
    "probs_vote  = voter.predict_proba(X_test)\n",
    "\n",
    "le = LabelEncoder().fit(y)                 # already in your script; reuse if present\n",
    "y_true_int = le.transform(y_test)          # integer labels aligned to le.classes_\n",
    "\n",
    "# Ensure columns align to le.classes_ order\n",
    "order = [np.where(voter.classes_ == c)[0][0] for c in le.classes_]\n",
    "probs_aligned = probs_vote[:, order]\n",
    "\n",
    "acc_top1  = top_k_accuracy_score(y_true_int, probs_aligned, k=1)\n",
    "acc_top3  = top_k_accuracy_score(y_true_int, probs_aligned, k=3)\n",
    "acc_top10 = top_k_accuracy_score(y_true_int, probs_aligned, k=min(10, probs_aligned.shape[1]))\n",
    "\n",
    "print(f\"SoftVoting â€” Top-1: {acc_top1:.4f} | Top-3: {acc_top3:.4f} | Top-10: {acc_top10:.4f}\")\n",
    "\n",
    "# Save the ensemble for serving\n",
    "joblib.dump({\n",
    "    \"model\": voter,\n",
    "    \"feature_names\": X.columns.tolist(),\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"num_cols\": num_cols,\n",
    "    \"label_encoder\": le\n",
    "}, \"serving_voter_topN.pkl\")\n",
    "\n",
    "print(\"âœ… Saved soft-voting model â†’ serving_voter_topN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded360b-af58-4d27-a292-ccca4dc29a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(results, columns=[\"Model\", \"Top3_Accuracy\"]).to_csv(COMPARE_CSV, index=False)\n",
    "\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    f.write(\"=== Top-3 Accuracy (k=3) ===\\n\")\n",
    "    for m,a in results:\n",
    "        f.write(f\"{m}: {a:.4f}\\n\")\n",
    "    f.write(\"\\n=== Soft Voting (extra metrics) ===\\n\")\n",
    "    f.write(f\"Top-1:  {acc_top1_vote:.4f}\\n\")\n",
    "    f.write(f\"Top-3:  {acc_top3_vote:.4f}\\n\")\n",
    "    f.write(f\"Top-10: {acc_top10_vote:.4f}\\n\")\n",
    "    f.write(\"\\nClasses:\\n\")\n",
    "    f.write(\", \".join(le.classes_))\n",
    "\n",
    "# ===================== PRINT =====================\n",
    "print(\"\\n=== Top-3 Accuracy (k=3) ===\")\n",
    "for m,a in results:\n",
    "    print(f\"{m:<16} : {a:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Soft Voting Ensemble (CatBoost + RF + GB + KNN)\")\n",
    "print(f\"Top-1  : {acc_top1_vote:.4f}\")\n",
    "print(f\"Top-3  : {acc_top3_vote:.4f}\")\n",
    "print(f\"Top-10 : {acc_top10_vote:.4f}\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - Solo CatBoost :\", MODEL_OUT_CAT)\n",
    "print(\" - Soft Voting   :\", MODEL_OUT_VOTE)\n",
    "print(\" - Report        :\", REPORT_OUT)\n",
    "print(\" - Compare CSV   :\", COMPARE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513b57e-927c-407f-8192-24eaaf54d1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
